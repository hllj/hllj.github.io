---
title: "Vista - Bộ dữ liệu Vision-Language cho Tiếng Việt"
categories: projects
classes: wide
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
toc_sticky: true
header:
  teaser: "/images/vista-thumb.png"
  overlay_image: "/images/vista-thumb.png"
  overlay_filter: 0.7
  overlay_color: "#005eff"
---

Dự án xây dựng dữ liệu Vision-Language cho tiếng Việt sử dụng synthetic data

# Project information

- [HuggingFace](https://huggingface.co/datasets/Vi-VLM/Vista)
- [Github](https://github.com/Oztobuzz/Vista/tree/main)

# Mở đầu

Project này được hai đứa em mình kêu gọi làm từ giữa năm 2024, hiện tại một người thì đang làm trong lab của VinUni, một người thì đang học PhD ở Pháp và làm việc cho Homebrew Research. Chuyện là sau khi tham gia buổi Google devfest về Gemini, hai đứa thấy Gemini đang có free 1 tháng để sử dụng API, và có ý tưởng để làm một bộ dữ liệu Multimodal cho tiếng Việt.

Mình tham gia dự án này và khá hứng thú về việc tạo ra một bộ dữ liệu Multimodal, thứ mình cũng tìm hiểu trong cả năm qua. Mình gợi ý rằng nên thử áp dụng cách làm dữ liệu từ LLaVA, một mô hình cũng theo hướng synthetic data từ OpenAI.

Từ đó mọi thành phần để làm dự án đã có:
- Tụi mình sử dụng Gemini để làm synthetic data.
- Sử dụng theo hướng của LLaVA để tạo prompt và tạo các instruction data cho multimodal.

# Dữ liệu

Mình đảm nhận các công việc đầu tiên về viết code sinh dữ liệu, cũng như đọc paper LLaVA. Mình thấy mô hình này sử dụng các dữ liệu có sẵn caption là COCO để tạo ra 158k mẫu dữ liệu từ 3 dạng instruction: conversation, detail description, complex reasoning.

Mình cũng tìm ra một bộ dữ liệu [dịch caption cho COCO tiếng Việt](https://huggingface.co/datasets/dinhanhx/coco-2017-vi) từ một repo trên HuggingFace.

Thêm vào đó, bên mình cũng tìm kiếm thêm một bộ có caption tiếng Việt khác là [WIT](https://huggingface.co/datasets/google/wit). Bộ này thì mô tả caption lúc chi tiết lúc không nhiều chi tiết, nhưng có rất nhiều thông tin đi kèm để có thể làm giàu chất lượng cho nó.

Ngoài ra, bên mình cũng tận dụng các bộ dữ liệu khác của các work khác có liên quan tới LLaVA như [ShareGPT4V](https://sharegpt4v.github.io/) và dịch nó sang tiếng Việt.

Để tóm tắt lại chúng mình sử dụng các dữ liệu:

- COCO2017 tiếng Việt để tạo ra các dạng instruction data conversation, detail description, complex reasoning.
- WIT: sử dụng các caption và thông tin đi kèm trong hình ảnh, ngoài ra áp dụng các dạng prompt chúng mình thử nghiệm được để tạo ra các conversation với hình ảnh.
- ShareGPT4V, chúng mình dịch ra từ bộ tiếng Anh gốc.

# Prompt

Chúng mình thử nghiệm một vài hướng prompting chủ yếu qua 2 phương án chính:

- LLaVA: tụi mình sử dụng 3 prompt cho 3 loại instruction data như mình đã nêu ở trên: conversation, detail description, complex reasoning.
- ShareGPT4V: tụi mình lưu ý cách sử dụng prompt cho từng dạng dữ liệu trong này, và áp dụng tương tự nó cho WIT và đem lại kết quả rất tốt.

- Prompt dịch tiếng Anh sang tiếng Việt để tận dụng các dữ liệu tiếng Anh.

# Code

Mình là người tạo ra code chính cho cả dự án với các phần việc để chạy cho dữ liệu COCO2017 và WIT.

Tụi mình gặp một số vấn đề về rate limit của Google chỉ cho phép 15req/min và mình nghĩ cách để sử dụng nhiều API key hơn chạy đồng thời.

Ngoài ra, cũng một vài phần nhằm lọc các dữ liệu kém chất lượng, ví dụ có lẫn thông tin đã đưa trong prompt vào câu trả lời, ...

Cuối cùng, bên mình có sử dụng một phần code để lọc lại theo perplexity để loại bỏ bớt các sample không nghe tự nhiên trong tiếng Việt.

# Kết thúc

Sau khi kết thúc dự án, chúng mình cũng đã hoàn thành được khá nhiều thứ đã đề ra:

- Tạo ra một bộ dữ liệu với hơn 700k mẫu dữ liệu hội thoại với hình ảnh, có mức độ phức tạp khá cao để huấn luyện.
- Tạo ra code base để tạo synthetic data với Gemini.

Tụi mình cũng thấy rằng cộng đồng đón nhận tích cực về đóng góp này và có sử dụng trong những dự án về multimodal. Đây là một trong những dự án mình thấy hài lòng nhất về mặt kết quả.

Dù một dự án nhỏ, thời gian thực hiện ngắn, nhưng mình thấy đã học được nhiều thứ và sẵn sàng cho những dự án trong tương lai.